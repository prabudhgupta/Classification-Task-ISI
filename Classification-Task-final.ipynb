{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funky-reform",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "### In this step the data is loaded and the 58th column is dropped because it was an empty column and 57th column is dropped from data and stored it in label as that's the column we need to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tested-italy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3     4     5     6     7     8     9  ...   47  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.0   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.0   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0   \n",
       "\n",
       "         48     49   50     51     52     53     54   55    56  \n",
       "0     0.000  0.000  0.0  0.778  0.000  0.000  3.756   61   278  \n",
       "1     0.000  0.132  0.0  0.372  0.180  0.048  5.114  101  1028  \n",
       "2     0.010  0.143  0.0  0.276  0.184  0.010  9.821  485  2259  \n",
       "3     0.000  0.137  0.0  0.137  0.000  0.000  3.537   40   191  \n",
       "4     0.000  0.135  0.0  0.135  0.000  0.000  3.537   40   191  \n",
       "...     ...    ...  ...    ...    ...    ...    ...  ...   ...  \n",
       "4596  0.000  0.232  0.0  0.000  0.000  0.000  1.142    3    88  \n",
       "4597  0.000  0.000  0.0  0.353  0.000  0.000  1.555    4    14  \n",
       "4598  0.102  0.718  0.0  0.000  0.000  0.000  1.404    6   118  \n",
       "4599  0.000  0.057  0.0  0.000  0.000  0.000  1.147    5    78  \n",
       "4600  0.000  0.000  0.0  0.125  0.000  0.000  1.250    5    40  \n",
       "\n",
       "[4601 rows x 57 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "file_path = \"spambase.data\"\n",
    "# above .data file is comma delimited\n",
    "data = pd.read_csv(file_path)\n",
    "data=data.drop(columns=['58'])\n",
    "label=data['57']\n",
    "data=data.drop(\"57\",axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-protection",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "### In this step all the modules necessary from sklearn are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesser-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.6.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "!pip install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-ultimate",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "### In this step a function is written to compute the ANOVA F-value for all the features in the given data, which is used in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continuous-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_classif, k='all')\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-closure",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "### In this steps 2 operations were done:\n",
    "    1. The data is split into training data and test data where 30% of the data is put in the test data and the remaining is the train data.\n",
    "    2. This splitted data is sent to the select_features function written in Step 3 to compute the F-values of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rough-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 53.395118\n",
      "Feature 1: 2.998980\n",
      "Feature 2: 146.713434\n",
      "Feature 3: 11.395680\n",
      "Feature 4: 211.586268\n",
      "Feature 5: 205.775203\n",
      "Feature 6: 438.071729\n",
      "Feature 7: 134.197377\n",
      "Feature 8: 179.047634\n",
      "Feature 9: 77.672430\n",
      "Feature 10: 159.498194\n",
      "Feature 11: 0.022849\n",
      "Feature 12: 57.601515\n",
      "Feature 13: 15.142624\n",
      "Feature 14: 141.069270\n",
      "Feature 15: 249.649446\n",
      "Feature 16: 240.368958\n",
      "Feature 17: 151.712474\n",
      "Feature 18: 254.472017\n",
      "Feature 19: 143.174068\n",
      "Feature 20: 557.988801\n",
      "Feature 21: 28.633145\n",
      "Feature 22: 406.841145\n",
      "Feature 23: 197.995684\n",
      "Feature 24: 237.770793\n",
      "Feature 25: 195.344638\n",
      "Feature 26: 113.067390\n",
      "Feature 27: 95.049683\n",
      "Feature 28: 71.420004\n",
      "Feature 29: 100.428004\n",
      "Feature 30: 52.882666\n",
      "Feature 31: 43.718224\n",
      "Feature 32: 57.573396\n",
      "Feature 33: 44.248088\n",
      "Feature 34: 71.698357\n",
      "Feature 35: 62.034506\n",
      "Feature 36: 109.184136\n",
      "Feature 37: 3.386501\n",
      "Feature 38: 54.322080\n",
      "Feature 39: 11.936257\n",
      "Feature 40: 26.658199\n",
      "Feature 41: 61.459365\n",
      "Feature 42: 56.612172\n",
      "Feature 43: 28.743614\n",
      "Feature 44: 62.479213\n",
      "Feature 45: 67.857103\n",
      "Feature 46: 5.832319\n",
      "Feature 47: 23.831938\n",
      "Feature 48: 12.028064\n",
      "Feature 49: 28.771923\n",
      "Feature 50: 10.349626\n",
      "Feature 51: 308.986966\n",
      "Feature 52: 416.479199\n",
      "Feature 53: 14.380455\n",
      "Feature 54: 49.663456\n",
      "Feature 55: 130.314356\n",
      "Feature 56: 206.186239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data,label,test_size=0.3)\n",
    "\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, Y_train, X_test)\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-evaluation",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "### In this step after splitting the data Knn classification is done for K= 1 to 40 neighbours and a graph between error rate and K is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amber-produce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data,label,test_size=0.2)\n",
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "        knn.fit(X_train,Y_train)\n",
    "        pred_i = knn.predict(X_test)\n",
    "        error_rate.append(np.mean(pred_i != Y_test))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n",
    "         marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-funds",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "### Converted the data and label to numpy for K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stock-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.to_numpy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrong-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-private",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "### In this step two functions are written:\n",
    "        1. get_matrix: To calculate the confusion matrix for the model\n",
    "        2. get_score: To calculate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dietary-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def get_matrix(model,X_train,X_test,Y_train,Y_test):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return confusion_matrix(Y_test,model.predict(X_test))\n",
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-dimension",
   "metadata": {},
   "source": [
    "# Step 8\n",
    "### Following operations are done:\n",
    "        1. For performing K-fold cross validation folds is initialized with K = 10. The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance\n",
    "        2. We are calculating scores for Logistic Regression, SVM, Random forest classifier, Naive Bayes and K nearest neighbours(neighbours=4) by changing the train and text index according to the folds.\n",
    "        3. From the results we can see that Random Forest classifier is the best of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "royal-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: Logistic Regression\n",
      "[0.93058568329718, 0.9282608695652174, 0.9173913043478261, 0.9456521739130435, 0.9369565217391305, 0.9304347826086956, 0.9521739130434783, 0.9391304347826087, 0.85, 0.8391304347826087]\n",
      "\n",
      "Scores: SVM \n",
      "[0.7787418655097614, 0.8, 0.8152173913043478, 0.8456521739130435, 0.8152173913043478, 0.8565217391304348, 0.8347826086956521, 0.8456521739130435, 0.75, 0.8021739130434783]\n",
      "\n",
      "Scores: Random Forest Classifier\n",
      "[0.9501084598698482, 0.9434782608695652, 0.9369565217391305, 0.9565217391304348, 0.9608695652173913, 0.9608695652173913, 0.9652173913043478, 0.9695652173913043, 0.8978260869565218, 0.8521739130434782]\n",
      "\n",
      "Scores: Naive Bayes\n",
      "[0.8438177874186551, 0.8630434782608696, 0.8782608695652174, 0.8673913043478261, 0.8847826086956522, 0.8282608695652174, 0.8326086956521739, 0.8673913043478261, 0.6347826086956522, 0.717391304347826]\n",
      "\n",
      "Scores: KNearestNeighbour\n",
      "[0.7483731019522777, 0.782608695652174, 0.7760869565217391, 0.7891304347826087, 0.8108695652173913, 0.8282608695652174, 0.7913043478260869, 0.7956521739130434, 0.7369565217391304, 0.7608695652173914]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "KClassifier = KNeighborsClassifier(n_neighbors=4)\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "scores_NB=[]\n",
    "scores_KNN=[]\n",
    "error_rate=[]\n",
    "matrix_rf=[]\n",
    "matrix_RF=[]\n",
    "for train_index, test_index in folds.split(data,label):\n",
    "    X_train, X_test, y_train, y_test = data[train_index], data[test_index], \\\n",
    "                                       label[train_index], label[test_index]\n",
    "    scores_logistic.append(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))\n",
    "    scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))\n",
    "    scores_NB.append(get_score(GaussianNB(), X_train, X_test, y_train, y_test))\n",
    "    scores_KNN.append(get_score(KClassifier, X_train, X_test, y_train, y_test))\n",
    "    \n",
    "print(\"Scores: Logistic Regression\")    \n",
    "print(scores_logistic)\n",
    "print(\"\")\n",
    "                     \n",
    "                         \n",
    "print(\"Scores: SVM \")    \n",
    "print(scores_svm)\n",
    "print(\"\")\n",
    "                     \n",
    "                         \n",
    "print(\"Scores: Random Forest Classifier\")    \n",
    "print(scores_rf)\n",
    "print(\"\")\n",
    "                     \n",
    "                     \n",
    "                         \n",
    "print(\"Scores: Naive Bayes\")    \n",
    "print(scores_NB)\n",
    "print(\"\")\n",
    "                     \n",
    "                         \n",
    "print(\"Scores: KNearestNeighbour\")    \n",
    "print(scores_KNN)\n",
    "print(\"\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-visit",
   "metadata": {},
   "source": [
    "# Step 9\n",
    "### Confusion matrix for all the models is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accessory-accountability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "matrix_rf=[]\n",
    "matrix_RF=[]\n",
    "matrix_nb=[]\n",
    "matrix_NB=[]\n",
    "matrix_knn=[]\n",
    "matrix_KNN=[]\n",
    "matrix_lr=[]\n",
    "matrix_LR=[]\n",
    "matrix_svm=[]\n",
    "matrix_SVM=[]\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(data,label):\n",
    "    X_train, X_test, y_train, y_test = data[train_index], data[test_index], \\\n",
    "                                       label[train_index], label[test_index]\n",
    "    matrix_rf=get_matrix(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test)\n",
    "    matrix_RF.append(matrix_rf)\n",
    "    \n",
    "    matrix_nb=get_matrix(GaussianNB(), X_train, X_test, y_train, y_test)\n",
    "    matrix_knn=get_matrix(KClassifier, X_train, X_test, y_train, y_test)\n",
    "    matrix_NB.append(matrix_nb)\n",
    "    matrix_KNN.append(matrix_knn)\n",
    "    \n",
    "    matrix_lr=get_matrix(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test)\n",
    "    matrix_svm=get_matrix(SVC(gamma='auto'), X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    matrix_LR.append(matrix_lr)\n",
    "    matrix_SVM.append(matrix_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-scanning",
   "metadata": {},
   "source": [
    "# Step 10\n",
    "### In this step the error rate, false positive and false negative rate for all the folds is calculated. And also the average error rate for RandomForestClassifier across all folds is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "confirmed-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_number(matrix,str):\n",
    "\n",
    "    data=[]\n",
    "    total_error_rate=0\n",
    "    for i in range (0,len(matrix)):\n",
    "        data.append([]) \n",
    "        data[i].append(i+1)\n",
    "        data[i].append(matrix[i][0][1])\n",
    "        data[i].append(matrix[i][1][0])\n",
    "        sum=data[i][1]+data[i][2]\n",
    "        total=matrix[i][0][1]+matrix[i][1][0]+matrix[i][0][0]+matrix[i][1][1]\n",
    "        error_rate=sum/total\n",
    "        data[i].append(error_rate)\n",
    "        total_error_rate=total_error_rate+error_rate\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_rate(matrix,str):\n",
    "    \n",
    "    data_rate=[]\n",
    "    total_error_rate=0\n",
    "    \n",
    "    for i in range (0,len(matrix)):\n",
    "        data_rate.append([]) \n",
    "        sum=matrix[i][0][1]+matrix[i][1][0]\n",
    "        total=matrix[i][0][1]+matrix[i][1][0]+matrix[i][0][0]+matrix[i][1][1]\n",
    "        error_rate=sum/total\n",
    "        total_error_rate=total_error_rate+error_rate\n",
    "        actual_no=matrix[i][0][1]+matrix[i][0][0]\n",
    "        actual_yes=matrix[i][1][0]+matrix[i][1][1]\n",
    "        data_rate[i].append(i+1)\n",
    "        fp_rate=matrix[i][0][1]/actual_no\n",
    "        fn_rate=matrix[i][1][0]/actual_yes\n",
    "        data_rate[i].append(fp_rate)\n",
    "        data_rate[i].append(fn_rate)\n",
    "        data_rate[i].append(error_rate)\n",
    "        \n",
    "        if str == 'rf' and i == len(matrix)-1:\n",
    "            average_error_rate=total_error_rate/10\n",
    "            data_rate.append([])\n",
    "            data_rate[i+1].append('average_error_rate')\n",
    "            data_rate[i+1].append('-')\n",
    "            data_rate[i+1].append('-')\n",
    "            data_rate[i+1].append(average_error_rate)\n",
    "            \n",
    "    return data_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "convertible-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lr=calculate_number(matrix_LR,'lr')\n",
    "data_knn=calculate_number(matrix_KNN,'knn')\n",
    "data_rf=calculate_number(matrix_RF,'rf')\n",
    "data_nb=calculate_number(matrix_NB,'nb')\n",
    "data_svm=calculate_number(matrix_SVM,'svm')\n",
    "\n",
    "data_rate_lr=calculate_rate(matrix_LR,'lr')\n",
    "data_rate_knn=calculate_rate(matrix_KNN,'knn')\n",
    "data_rate_rf=calculate_rate(matrix_RF,'rf')\n",
    "data_rate_nb=calculate_rate(matrix_NB,'nb')\n",
    "data_rate_svm=calculate_rate(matrix_SVM,'svm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-causing",
   "metadata": {},
   "source": [
    "# Step 11\n",
    "### In this step false positives, false negatives are printed for all the folds of all models except RandomForest along with the error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "diagnostic-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------LOGISTIC REGRESSION-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                 8                24     0.0694143\n",
      "     2                10                23     0.0717391\n",
      "     3                16                22     0.0826087\n",
      "     4                12                13     0.0543478\n",
      "     5                13                16     0.0630435\n",
      "     6                22                10     0.0695652\n",
      "     7                 4                18     0.0478261\n",
      "     8                12                16     0.0608696\n",
      "     9                43                26     0.15\n",
      "    10                37                37     0.16087\n",
      " \n",
      "  Fold    False Positive rate    False Negative rate    Error rate\n",
      "------  ---------------------  ---------------------  ------------\n",
      "     1              0.0286738              0.131868      0.0694143\n",
      "     2              0.0359712              0.126374      0.0717391\n",
      "     3              0.057554               0.120879      0.0826087\n",
      "     4              0.0430108              0.0718232     0.0543478\n",
      "     5              0.046595               0.0883978     0.0630435\n",
      "     6              0.078853               0.0552486     0.0695652\n",
      "     7              0.0143369              0.0994475     0.0478261\n",
      "     8              0.0430108              0.0883978     0.0608696\n",
      "     9              0.154122               0.143646      0.15\n",
      "    10              0.132616               0.20442       0.16087\n",
      "\n",
      "--------KNearestNeighbour-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                37                79      0.251627\n",
      "     2                32                68      0.217391\n",
      "     3                30                73      0.223913\n",
      "     4                21                76      0.21087\n",
      "     5                29                58      0.18913\n",
      "     6                32                47      0.171739\n",
      "     7                20                76      0.208696\n",
      "     8                31                63      0.204348\n",
      "     9                55                66      0.263043\n",
      "    10                44                66      0.23913\n",
      " \n",
      "  Fold    False Positive rate    False Negative rate    Error rate\n",
      "------  ---------------------  ---------------------  ------------\n",
      "     1              0.132616                0.434066      0.251627\n",
      "     2              0.115108                0.373626      0.217391\n",
      "     3              0.107914                0.401099      0.223913\n",
      "     4              0.0752688               0.41989       0.21087\n",
      "     5              0.103943                0.320442      0.18913\n",
      "     6              0.114695                0.259669      0.171739\n",
      "     7              0.0716846               0.41989       0.208696\n",
      "     8              0.111111                0.348066      0.204348\n",
      "     9              0.197133                0.364641      0.263043\n",
      "    10              0.157706                0.364641      0.23913\n",
      "\n",
      "--------Naive Bayes-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                65                 7      0.156182\n",
      "     2                59                 4      0.136957\n",
      "     3                52                 4      0.121739\n",
      "     4                58                 3      0.132609\n",
      "     5                43                10      0.115217\n",
      "     6                77                 2      0.171739\n",
      "     7                72                 5      0.167391\n",
      "     8                50                11      0.132609\n",
      "     9               157                11      0.365217\n",
      "    10               109                21      0.282609\n",
      " \n",
      "  Fold    False Positive rate    False Negative rate    Error rate\n",
      "------  ---------------------  ---------------------  ------------\n",
      "     1               0.232975              0.0384615      0.156182\n",
      "     2               0.21223               0.021978       0.136957\n",
      "     3               0.18705               0.021978       0.121739\n",
      "     4               0.207885              0.0165746      0.132609\n",
      "     5               0.154122              0.0552486      0.115217\n",
      "     6               0.275986              0.0110497      0.171739\n",
      "     7               0.258065              0.0276243      0.167391\n",
      "     8               0.179211              0.0607735      0.132609\n",
      "     9               0.562724              0.0607735      0.365217\n",
      "    10               0.390681              0.116022       0.282609\n",
      "\n",
      "--------SVM-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                58                44      0.221258\n",
      "     2                46                46      0.2\n",
      "     3                42                43      0.184783\n",
      "     4                34                37      0.154348\n",
      "     5                51                34      0.184783\n",
      "     6                45                21      0.143478\n",
      "     7                33                43      0.165217\n",
      "     8                41                30      0.154348\n",
      "     9                79                36      0.25\n",
      "    10                55                36      0.197826\n",
      " \n",
      "  Fold    False Positive rate    False Negative rate    Error rate\n",
      "------  ---------------------  ---------------------  ------------\n",
      "     1               0.207885               0.241758      0.221258\n",
      "     2               0.165468               0.252747      0.2\n",
      "     3               0.151079               0.236264      0.184783\n",
      "     4               0.121864               0.20442       0.154348\n",
      "     5               0.182796               0.187845      0.184783\n",
      "     6               0.16129                0.116022      0.143478\n",
      "     7               0.11828                0.237569      0.165217\n",
      "     8               0.146953               0.165746      0.154348\n",
      "     9               0.283154               0.198895      0.25\n",
      "    10               0.197133               0.198895      0.197826\n",
      "\n",
      "--------The Best Performing Model is RandomForestClassifier-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                 7                20     0.0585683\n",
      "     2                 7                16     0.05\n",
      "     3                 4                26     0.0652174\n",
      "     4                 6                21     0.0586957\n",
      "     5                 5                16     0.0456522\n",
      "     6                13                 8     0.0456522\n",
      "     7                 3                14     0.0369565\n",
      "     8                 3                 9     0.026087\n",
      "     9                35                17     0.113043\n",
      "    10                31                35     0.143478\n",
      " \n",
      "Fold                False Positive rate    False Negative rate      Error rate\n",
      "------------------  ---------------------  ---------------------  ------------\n",
      "1                   0.025089605734767026   0.10989010989010989       0.0585683\n",
      "2                   0.025179856115107913   0.08791208791208792       0.05\n",
      "3                   0.014388489208633094   0.14285714285714285       0.0652174\n",
      "4                   0.021505376344086023   0.11602209944751381       0.0586957\n",
      "5                   0.017921146953405017   0.08839779005524862       0.0456522\n",
      "6                   0.04659498207885305    0.04419889502762431       0.0456522\n",
      "7                   0.010752688172043012   0.07734806629834254       0.0369565\n",
      "8                   0.010752688172043012   0.049723756906077346      0.026087\n",
      "9                   0.12544802867383512    0.09392265193370165       0.113043\n",
      "10                  0.1111111111111111     0.19337016574585636       0.143478\n",
      "average_error_rate  -                      -                         0.0643351\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--------LOGISTIC REGRESSION-------\")\n",
    "print(\" \")\n",
    "print (tabulate(data_lr, headers=[\"Fold\",\"False Positive\", \"False Negative\", \"Error rate\"]))\n",
    "print(\" \")\n",
    "print (tabulate(data_rate_lr, headers=[\"Fold\",\"False Positive rate\", \"False Negative rate\", \"Error rate\"]))\n",
    "print(\"\")\n",
    "\n",
    "print(\"--------KNearestNeighbour-------\")\n",
    "print(\" \")\n",
    "print (tabulate(data_knn, headers=[\"Fold\",\"False Positive\", \"False Negative\", \"Error rate\"]))\n",
    "print(\" \")\n",
    "print (tabulate(data_rate_knn, headers=[\"Fold\",\"False Positive rate\", \"False Negative rate\", \"Error rate\"]))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------Naive Bayes-------\")\n",
    "print(\" \")\n",
    "print (tabulate(data_nb, headers=[\"Fold\",\"False Positive\", \"False Negative\", \"Error rate\"]))\n",
    "print(\" \")\n",
    "print (tabulate(data_rate_nb, headers=[\"Fold\",\"False Positive rate\", \"False Negative rate\", \"Error rate\"]))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------SVM-------\")\n",
    "print(\" \")\n",
    "print (tabulate(data_svm, headers=[\"Fold\",\"False Positive\", \"False Negative\", \"Error rate\"]))\n",
    "print(\" \")\n",
    "print (tabulate(data_rate_svm, headers=[\"Fold\",\"False Positive rate\", \"False Negative rate\", \"Error rate\"]))\n",
    "print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-combine",
   "metadata": {},
   "source": [
    "# Step 12\n",
    "### In this step false positives, false negatives are printed for all the folds of RandomForest along with the error rate and average error rate . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "literary-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------The Best Performing Model is RandomForestClassifier-------\n",
      " \n",
      "  Fold    False Positive    False Negative    Error rate\n",
      "------  ----------------  ----------------  ------------\n",
      "     1                 7                20     0.0585683\n",
      "     2                 7                16     0.05\n",
      "     3                 4                26     0.0652174\n",
      "     4                 6                21     0.0586957\n",
      "     5                 5                16     0.0456522\n",
      "     6                13                 8     0.0456522\n",
      "     7                 3                14     0.0369565\n",
      "     8                 3                 9     0.026087\n",
      "     9                35                17     0.113043\n",
      "    10                31                35     0.143478\n",
      " \n",
      "Fold                False Positive rate    False Negative rate      Error rate\n",
      "------------------  ---------------------  ---------------------  ------------\n",
      "1                   0.025089605734767026   0.10989010989010989       0.0585683\n",
      "2                   0.025179856115107913   0.08791208791208792       0.05\n",
      "3                   0.014388489208633094   0.14285714285714285       0.0652174\n",
      "4                   0.021505376344086023   0.11602209944751381       0.0586957\n",
      "5                   0.017921146953405017   0.08839779005524862       0.0456522\n",
      "6                   0.04659498207885305    0.04419889502762431       0.0456522\n",
      "7                   0.010752688172043012   0.07734806629834254       0.0369565\n",
      "8                   0.010752688172043012   0.049723756906077346      0.026087\n",
      "9                   0.12544802867383512    0.09392265193370165       0.113043\n",
      "10                  0.1111111111111111     0.19337016574585636       0.143478\n",
      "average_error_rate  -                      -                         0.0643351\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"--------The Best Performing Model is RandomForestClassifier-------\")\n",
    "print(\" \")\n",
    "print (tabulate(data_rf, headers=[\"Fold\",\"False Positive\", \"False Negative\", \"Error rate\"]))\n",
    "print(\" \")\n",
    "print (tabulate(data_rate_rf, headers=[\"Fold\",\"False Positive rate\", \"False Negative rate\", \"Error rate\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
